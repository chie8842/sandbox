{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ベース"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-1ae4497511aa>:125: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Epoch: 0001 cost= 533.373213249\n",
      "Epoch: 0002 cost= 84.698960343\n",
      "Epoch: 0003 cost= 22.134174088\n",
      "Epoch: 0004 cost= 5.108635333\n",
      "Epoch: 0005 cost= 2.263418975\n",
      "Epoch: 0006 cost= 1.333921925\n",
      "Epoch: 0007 cost= 0.892475556\n",
      "Epoch: 0008 cost= 0.681980373\n",
      "Epoch: 0009 cost= 0.596151590\n",
      "Epoch: 0010 cost= 0.555017511\n",
      "Optimization Finished!\n",
      "Accuracy: 0.835165\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001 # 学習率 高いとcostの収束が早まる\n",
    "training_epochs = 10 # 学習全体をこのエポック数で区切り、区切りごとにcostを表示する\n",
    "batch_size = 100     # 学習1回ごと( sess.run()ごと )に訓練データをいくつ利用するか\n",
    "display_step = 1     # 1なら毎エポックごとにcostを表示\n",
    "train_size = 800     # 全データの中でいくつ訓練データに回すか\n",
    "step_size = 2000     # 何ステップ学習するか\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 64      # 隠れ層1のユニットの数\n",
    "n_hidden_2 = 64      # 隠れ層2のユニットの数\n",
    "n_input = 8          # 与える変数の数\n",
    "n_classes = 2        # 分類するクラスの数 今回は生き残ったか否かなので2\n",
    "\n",
    "\n",
    "\n",
    "data_dir = \"/Users/chiehayashida/work/kaggle/titanic\"\n",
    "train_file = data_dir + '/train.csv'\n",
    "test_file = data_dir + '/test.csv'\n",
    "\n",
    "# データの読み込み\n",
    "train = pd.read_csv(train_file)\n",
    "test = pd.read_csv(test_file)\n",
    "\n",
    "all = pd.concat([train, test])\n",
    "\n",
    "\n",
    "# 値の欠落を埋める\n",
    "train[\"Age\"].fillna(train.Age.median(), inplace=True) \n",
    "train[\"Fare\"].fillna(train.Age.median(), inplace=True) \n",
    "train.Cabin.fillna(\"NaN\", inplace=True)\n",
    "train.Embarked.fillna(\"NaN\", inplace=True)\n",
    "\n",
    "test[\"Age\"].fillna(train.Age.median(), inplace=True) \n",
    "test[\"Fare\"].fillna(train.Age.median(), inplace=True) \n",
    "test.Cabin.fillna(\"NaN\", inplace=True)\n",
    "test.Embarked.fillna(\"NaN\", inplace=True)\n",
    "\n",
    "all = pd.concat([train, test])\n",
    "\n",
    "# ラベルエンコーダの宣言\n",
    "le_sex_tr = preprocessing.LabelEncoder()\n",
    "le_cabin_tr = preprocessing.LabelEncoder()\n",
    "le_embarked_tr = preprocessing.LabelEncoder()\n",
    "\n",
    "le_sex_model = le_sex_tr.fit(all['Sex'])\n",
    "le_cabin_model = le_cabin_tr.fit(all['Cabin'])\n",
    "le_embarked_model = le_embarked_tr.fit(all['Embarked'])\n",
    "\n",
    "# ラベルエンコーディング\n",
    "train['Sex'] = le_sex_model.transform(train['Sex'])\n",
    "train['Cabin'] = le_cabin_tr.transform(train.Cabin)\n",
    "train['Embarked'] = le_embarked_tr.transform(train.Embarked)\n",
    "\n",
    "test['Sex'] = le_sex_model.transform(test['Sex'])\n",
    "test['Cabin'] = le_cabin_tr.transform(test.Cabin)\n",
    "test['Embarked'] = le_embarked_tr.transform(test.Embarked)\n",
    "\n",
    "# 特徴量データをnp.arrayに整形\n",
    "x_np = train[['Age', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']].values\n",
    "x_test_np = test[['Age', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']].values.astype(np.float32)\n",
    "\n",
    "# クラスデータをOneHotエンコーディングしてnp.arrayに整形\n",
    "oe = preprocessing.OneHotEncoder(sparse=False)\n",
    "y_np= oe.fit_transform(train[['Survived']])\n",
    "\n",
    "# 訓練データと試験データに分ける\n",
    "[x_train, x_test] = np.vsplit(x_np, [train_size])\n",
    "[y_train, y_test] = np.vsplit(y_np, [train_size])\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "x_ = tf.constant(x_test_np)\n",
    "#y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "y_ = tf.placeholder(\"float\", [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# ニューラルネットワークによるモデルの定義\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    # dropout\n",
    "\n",
    "    layer_1_drop = tf.nn.dropout(layer_1, keep_prob)\n",
    "    \n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1_drop, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# 重みとバイアスの定義\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# 予測値の定義\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# 損失関数とオプティマイザの定義\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# 値の初期化\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "test_prediction = tf.nn.softmax(multilayer_perceptron(x_, weights, biases))\n",
    "\n",
    "# 学習実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "\n",
    "        # Loop over step_size\n",
    "        for i in range(step_size):\n",
    "            # 訓練データから batch_size で指定した数をランダムに取得\n",
    "            ind = np.random.choice(train_size, batch_size)\n",
    "            x_train_batch = x_np[ind]\n",
    "            y_train_batch = y_np[ind]\n",
    "            x_train_batch\n",
    "    # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: x_train_batch, y: y_train_batch, keep_prob: 0.5})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / step_size\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, \"model.ckpt\")\n",
    "    print(\"Optimization Finished!\")\n",
    "    #test_prediction.eval()\n",
    "\n",
    "   ## モデルの評価\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: x_test, y: y_test, keep_prob: 1.0}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# test_prediction.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cabin抜いて学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 8.717519956\n",
      "Epoch: 0002 cost= 1.593550263\n",
      "Epoch: 0003 cost= 1.302829687\n",
      "Epoch: 0004 cost= 1.150671896\n",
      "Epoch: 0005 cost= 1.058887725\n",
      "Epoch: 0006 cost= 1.022771144\n",
      "Epoch: 0007 cost= 1.038750036\n",
      "Epoch: 0008 cost= 0.939835104\n",
      "Epoch: 0009 cost= 0.956257885\n",
      "Epoch: 0010 cost= 0.959241067\n",
      "Optimization Finished!\n",
      "Accuracy: 0.78022\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001 # 学習率 高いとcostの収束が早まる\n",
    "training_epochs = 10 # 学習全体をこのエポック数で区切り、区切りごとにcostを表示する\n",
    "batch_size = 100     # 学習1回ごと( sess.run()ごと )に訓練データをいくつ利用するか\n",
    "display_step = 1     # 1なら毎エポックごとにcostを表示\n",
    "train_size = 800     # 全データの中でいくつ訓練データに回すか\n",
    "step_size = 2000     # 何ステップ学習するか\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 64      # 隠れ層1のユニットの数\n",
    "n_hidden_2 = 64      # 隠れ層2のユニットの数\n",
    "n_input = 7          # 与える変数の数\n",
    "n_classes = 2        # 分類するクラスの数 今回は生き残ったか否かなので2\n",
    "\n",
    "\n",
    "\n",
    "data_dir = \"/home/chie8842/share/titanic\"\n",
    "train_file = data_dir + '/train.csv'\n",
    "test_file = data_dir + '/test.csv'\n",
    "\n",
    "# データの読み込み\n",
    "train = pd.read_csv(train_file)\n",
    "test = pd.read_csv(test_file)\n",
    "\n",
    "\n",
    "# 値の欠落を埋める\n",
    "train[\"Age\"].fillna(train.Age.median(), inplace=True) \n",
    "train[\"Fare\"].fillna(train.Age.median(), inplace=True) \n",
    "train.Cabin.fillna(\"NaN\", inplace=True)\n",
    "train.Embarked.fillna(\"NaN\", inplace=True)\n",
    "\n",
    "# ラベルエンコーダの宣言\n",
    "le_sex_tr = preprocessing.LabelEncoder()\n",
    "le_cabin_tr = preprocessing.LabelEncoder()\n",
    "le_embarked_tr = preprocessing.LabelEncoder()\n",
    "\n",
    "# ラベルエンコーディング\n",
    "train['Sex'] = le_sex_tr.fit_transform(train['Sex'])\n",
    "train['Cabin'] = le_cabin_tr.fit_transform(train.Cabin)\n",
    "train['Embarked'] = le_embarked_tr.fit_transform(train.Embarked)\n",
    "\n",
    "# 特徴量データをnp.arrayに整形\n",
    "x_np = train[['Age', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked']].values\n",
    "\n",
    "# クラスデータをOneHotエンコーディングしてnp.arrayに整形\n",
    "oe = preprocessing.OneHotEncoder(sparse=False)\n",
    "y_np = oe.fit_transform(train[['Survived']])\n",
    "\n",
    "# 訓練データと試験データに分ける\n",
    "[x_train, x_test] = np.vsplit(x_np, [train_size])\n",
    "[y_train, y_test] = np.vsplit(y_np, [train_size])\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "#y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# ニューラルネットワークによるモデルの定義\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    # dropout\n",
    "\n",
    "    layer_1_drop = tf.nn.dropout(layer_1, keep_prob)\n",
    "    \n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1_drop, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# 重みとバイアスの定義\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# 予測値の定義\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# 損失関数とオプティマイザの定義\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# 値の初期化\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# 学習実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "\n",
    "        # Loop over step_size\n",
    "        for i in range(step_size):\n",
    "            # 訓練データから batch_size で指定した数をランダムに取得\n",
    "            ind = np.random.choice(train_size, batch_size)\n",
    "            x_train_batch = x_train[ind]\n",
    "            y_train_batch = y_train[ind]\n",
    "            x_train_batch\n",
    "    # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: x_train_batch, y: y_train_batch, keep_prob: 1.0})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / step_size\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # モデルの評価\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: x_test, y: y_test, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ageが入ってないの抜いて学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-801d1a01770a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001 # 学習率 高いとcostの収束が早まる\n",
    "training_epochs = 10 # 学習全体をこのエポック数で区切り、区切りごとにcostを表示する\n",
    "batch_size = 100     # 学習1回ごと( sess.run()ごと )に訓練データをいくつ利用するか\n",
    "display_step = 1     # 1なら毎エポックごとにcostを表示\n",
    "train_size = 650     # 全データの中でいくつ訓練データに回すか\n",
    "step_size = 2000     # 何ステップ学習するか\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 64      # 隠れ層1のユニットの数\n",
    "n_hidden_2 = 64      # 隠れ層2のユニットの数\n",
    "n_input = 8          # 与える変数の数\n",
    "n_classes = 2        # 分類するクラスの数 今回は生き残ったか否かなので2\n",
    "\n",
    "data_dir = \"/home/chie8842/share/titanic\"\n",
    "train_file = data_dir + '/train.csv'\n",
    "test_file = data_dir + '/test.csv'\n",
    "\n",
    "# データの読み込み\n",
    "train = pd.read_csv(train_file)\n",
    "test = pd.read_csv(test_file)\n",
    "\n",
    "# 値の欠落を埋める\n",
    "train[\"Fare\"].fillna(train.Age.median(), inplace=True) \n",
    "train.Cabin.fillna(\"NaN\", inplace=True)\n",
    "train.Embarked.fillna(\"NaN\", inplace=True)\n",
    "\n",
    "# ラベルエンコーダの宣言\n",
    "le_sex_tr = preprocessing.LabelEncoder()\n",
    "le_cabin_tr = preprocessing.LabelEncoder()\n",
    "le_embarked_tr = preprocessing.LabelEncoder()\n",
    "\n",
    "# ラベルエンコーディング\n",
    "train['Sex'] = le_sex_tr.fit_transform(train['Sex'])\n",
    "train['Cabin'] = le_cabin_tr.fit_transform(train.Cabin)\n",
    "train['Embarked'] = le_embarked_tr.fit_transform(train.Embarked)\n",
    "\n",
    "age_mean = train['Age'].mean()\n",
    "n = np.nan\n",
    "train_AgeNa = train.query('(Age != Age)')\n",
    "train_AgeNa.fillna(math.floor(age_mean), inplace=True)\n",
    "\n",
    "\n",
    "train = train.dropna(subset=['Age'])\n",
    "\n",
    "# 特徴量データをnp.arrayに整形\n",
    "x_np = train[['Age', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']].values\n",
    "x_np_agena = train_AgeNa[['Age', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']].values\n",
    "\n",
    "# クラスデータをOneHotエンコーディングしてnp.arrayに整形\n",
    "oe = preprocessing.OneHotEncoder(sparse=False)\n",
    "y_np = oe.fit_transform(train[['Survived']])\n",
    "y_np_agena = oe.transform(train_AgeNa[['Survived']])\n",
    "\n",
    "# 訓練データと試験データに分ける\n",
    "[x_train, x_test] = np.vsplit(x_np, [train_size])\n",
    "[y_train, y_test] = np.vsplit(y_np, [train_size])\n",
    "\n",
    "x_test = np.concatenate((x_test, x_np_agena), axis=0)\n",
    "y_test = np.concatenate((y_test, y_np_agena), axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 22.638369894\n",
      "Epoch: 0002 cost= 2.460456284\n",
      "Epoch: 0003 cost= 2.132185943\n",
      "Epoch: 0004 cost= 1.968187981\n",
      "Epoch: 0005 cost= 1.804385882\n",
      "Epoch: 0006 cost= 1.824740359\n",
      "Epoch: 0007 cost= 1.923468604\n",
      "Epoch: 0008 cost= 1.802301973\n",
      "Epoch: 0009 cost= 1.833400029\n",
      "Epoch: 0010 cost= 1.731921375\n",
      "Optimization Finished!\n",
      "Accuracy: 0.804979\n"
     ]
    }
   ],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "#y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# ニューラルネットワークによるモデルの定義\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    # dropout\n",
    "\n",
    "    layer_1_drop = tf.nn.dropout(layer_1, keep_prob)\n",
    "    \n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1_drop, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# 重みとバイアスの定義\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# 予測値の定義\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# 損失関数とオプティマイザの定義\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# 値の初期化\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# 学習実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "\n",
    "        # Loop over step_size\n",
    "        for i in range(step_size):\n",
    "            # 訓練データから batch_size で指定した数をランダムに取得\n",
    "            ind = np.random.choice(train_size, batch_size)\n",
    "            x_train_batch = x_train[ind]\n",
    "            y_train_batch = y_train[ind]\n",
    "            x_train_batch\n",
    "    # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: x_train_batch, y: y_train_batch, keep_prob: 1.0})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / step_size\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # モデルの評価\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: x_test, y: y_test, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 年齢がわからない人は99才にする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 50.103320393\n",
      "Epoch: 0002 cost= 4.729443583\n",
      "Epoch: 0003 cost= 4.216191870\n",
      "Epoch: 0004 cost= 3.938698293\n",
      "Epoch: 0005 cost= 4.128632157\n",
      "Epoch: 0006 cost= 4.122880921\n",
      "Epoch: 0007 cost= 3.837393814\n",
      "Epoch: 0008 cost= 3.330383232\n",
      "Epoch: 0009 cost= 3.865558344\n",
      "Epoch: 0010 cost= 3.392739815\n",
      "Optimization Finished!\n",
      "Accuracy: 0.759336\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001 # 学習率 高いとcostの収束が早まる\n",
    "training_epochs = 10 # 学習全体をこのエポック数で区切り、区切りごとにcostを表示する\n",
    "batch_size = 100     # 学習1回ごと( sess.run()ごと )に訓練データをいくつ利用するか\n",
    "display_step = 1     # 1なら毎エポックごとにcostを表示\n",
    "train_size = 650     # 全データの中でいくつ訓練データに回すか\n",
    "step_size = 2000     # 何ステップ学習するか\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 64      # 隠れ層1のユニットの数\n",
    "n_hidden_2 = 64      # 隠れ層2のユニットの数\n",
    "n_input = 8          # 与える変数の数\n",
    "n_classes = 2        # 分類するクラスの数 今回は生き残ったか否かなので2\n",
    "data_dir = \"/home/chie8842/share/titanic\"\n",
    "train_file = data_dir + '/train.csv'\n",
    "test_file = data_dir + '/test.csv'\n",
    "\n",
    "# データの読み込み\n",
    "train = pd.read_csv(train_file)\n",
    "test = pd.read_csv(test_file)\n",
    "\n",
    "\n",
    "# 値の欠落を埋める\n",
    "train[\"Age\"].fillna(99, inplace=True) \n",
    "train[\"Fare\"].fillna(train.Age.median(), inplace=True) \n",
    "train.Cabin.fillna(\"NaN\", inplace=True)\n",
    "train.Embarked.fillna(\"NaN\", inplace=True)\n",
    "\n",
    "\n",
    "# ラベルエンコーダの宣言\n",
    "le_sex_tr = preprocessing.LabelEncoder()\n",
    "le_cabin_tr = preprocessing.LabelEncoder()\n",
    "le_embarked_tr = preprocessing.LabelEncoder()\n",
    "\n",
    "# ラベルエンコーディング\n",
    "train['Sex'] = le_sex_tr.fit_transform(train['Sex'])\n",
    "train['Cabin'] = le_cabin_tr.fit_transform(train.Cabin)\n",
    "train['Embarked'] = le_embarked_tr.fit_transform(train.Embarked)\n",
    "\n",
    "# 特徴量データをnp.arrayに整形\n",
    "x_np = train[['Age', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']].values\n",
    "\n",
    "# クラスデータをOneHotエンコーディングしてnp.arrayに整形\n",
    "oe = preprocessing.OneHotEncoder(sparse=False)\n",
    "y_np = oe.fit_transform(train[['Survived']])\n",
    "\n",
    "# 訓練データと試験データに分ける\n",
    "[x_train, x_test] = np.vsplit(x_np, [train_size])\n",
    "[y_train, y_test] = np.vsplit(y_np, [train_size])\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "#y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# ニューラルネットワークによるモデルの定義\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    # dropout\n",
    "\n",
    "    layer_1_drop = tf.nn.dropout(layer_1, keep_prob)\n",
    "    \n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1_drop, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# 重みとバイアスの定義\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# 予測値の定義\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# 損失関数とオプティマイザの定義\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# 値の初期化\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# 学習実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "\n",
    "        # Loop over step_size\n",
    "        for i in range(step_size):\n",
    "            # 訓練データから batch_size で指定した数をランダムに取得\n",
    "            ind = np.random.choice(train_size, batch_size)\n",
    "            x_train_batch = x_train[ind]\n",
    "            y_train_batch = y_train[ind]\n",
    "            x_train_batch\n",
    "    # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: x_train_batch, y: y_train_batch, keep_prob: 1.0})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / step_size\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # モデルの評価\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: x_test, y: y_test, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 年齢も予測する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chie8842/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1, 714]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-aa252cd0e3ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m#train_dropnaage.dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mlr_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;31m#output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/chie8842/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0mn_jobs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[0;32m--> 512\u001b[0;31m                          y_numeric=True, multi_output=True)\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/chie8842/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/chie8842/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 181\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1, 714]"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001 # 学習率 高いとcostの収束が早まる\n",
    "training_epochs = 10 # 学習全体をこのエポック数で区切り、区切りごとにcostを表示する\n",
    "batch_size = 100     # 学習1回ごと( sess.run()ごと )に訓練データをいくつ利用するか\n",
    "display_step = 1     # 1なら毎エポックごとにcostを表示\n",
    "train_size = 650     # 全データの中でいくつ訓練データに回すか\n",
    "step_size = 2000     # 何ステップ学習するか\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 64      # 隠れ層1のユニットの数\n",
    "n_hidden_2 = 64      # 隠れ層2のユニットの数\n",
    "n_input = 8          # 与える変数の数\n",
    "n_classes = 2        # 分類するクラスの数 今回は生き残ったか否かなので2\n",
    "\n",
    "data_dir = \"/home/chie8842/share/titanic\"\n",
    "train_file = data_dir + '/train.csv'\n",
    "test_file = data_dir + '/test.csv'\n",
    "\n",
    "# データの読み込み\n",
    "train = pd.read_csv(train_file)\n",
    "test = pd.read_csv(test_file)\n",
    "\n",
    "# 値の欠落を埋める\n",
    "train[\"Fare\"].fillna(train.Age.median(), inplace=True) \n",
    "train.Cabin.fillna(\"NaN\", inplace=True)\n",
    "train.Embarked.fillna(\"NaN\", inplace=True)\n",
    "\n",
    "# ラベルエンコーダの宣言\n",
    "le_sex_tr = preprocessing.LabelEncoder()\n",
    "le_cabin_tr = preprocessing.LabelEncoder()\n",
    "le_embarked_tr = preprocessing.LabelEncoder()\n",
    "\n",
    "# ラベルエンコーディング\n",
    "train['Sex'] = le_sex_tr.fit_transform(train['Sex'])\n",
    "train['Cabin'] = le_cabin_tr.fit_transform(train.Cabin)\n",
    "train['Embarked'] = le_embarked_tr.fit_transform(train.Embarked)\n",
    "\n",
    "train_dropnaage = train.dropna(subset=['Age'])\n",
    "#[['Age', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']]\n",
    "\n",
    "train_nanage = train.query('(Age != Age)')\n",
    "\n",
    "model = linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)\n",
    "lr_x = train_dropnaage.values[:,6].reshape(-1,1)\n",
    "lr_y = train_dropnaage.values[:,5].astype(int).reshape(-1,1)\n",
    "#train_dropnaage.dtypes\n",
    "#lr_y\n",
    "output = model.fit(lr_x,lr_y)\n",
    "\n",
    "lr_t_x = train_nanage.values[:,6].reshape(-1,1)\n",
    "lr_t_y = model.predict(lr_t_x)\n",
    "\n",
    "#output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "age_mean = train['Age'].mean()\n",
    "n = np.nan\n",
    "train_AgeNa = train.query('(Age != Age)')\n",
    "train_AgeNa.fillna(math.floor(age_mean), inplace=True)\n",
    "\n",
    "\n",
    "train = train.dropna(subset=['Age'])\n",
    "\n",
    "# 特徴量データをnp.arrayに整形\n",
    "x_np = train[['Age', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']].values\n",
    "x_np_agena = train_AgeNa[['Age', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']].values\n",
    "\n",
    "# クラスデータをOneHotエンコーディングしてnp.arrayに整形\n",
    "oe = preprocessing.OneHotEncoder(sparse=False)\n",
    "y_np = oe.fit_transform(train[['Survived']])\n",
    "y_np_agena = oe.transform(train_AgeNa[['Survived']])\n",
    "\n",
    "# 訓練データと試験データに分ける\n",
    "[x_train, x_test] = np.vsplit(x_np, [train_size])\n",
    "[y_train, y_test] = np.vsplit(y_np, [train_size])\n",
    "\n",
    "x_test = np.concatenate((x_test, x_np_agena), axis=0)\n",
    "y_test = np.concatenate((y_test, y_np_agena), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41999999999999998"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001 # 学習率 高いとcostの収束が早まる\n",
    "training_epochs = 10 # 学習全体をこのエポック数で区切り、区切りごとにcostを表示する\n",
    "batch_size = 100     # 学習1回ごと( sess.run()ごと )に訓練データをいくつ利用するか\n",
    "display_step = 1     # 1なら毎エポックごとにcostを表示\n",
    "train_size = 800     # 全データの中でいくつ訓練データに回すか\n",
    "step_size = 2000     # 何ステップ学習するか\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 64      # 隠れ層1のユニットの数\n",
    "n_hidden_2 = 64      # 隠れ層2のユニットの数\n",
    "n_input = 8          # 与える変数の数\n",
    "n_classes = 2        # 分類するクラスの数 今回は生き残ったか否かなので2\n",
    "\n",
    "\n",
    "\n",
    "data_dir = \"/home/chie8842/share/titanic\"\n",
    "train_file = data_dir + '/train.csv'\n",
    "test_file = data_dir + '/test.csv'\n",
    "\n",
    "# データの読み込み\n",
    "train = pd.read_csv(train_file)\n",
    "test = pd.read_csv(test_file)\n",
    "\n",
    "#org_test.dropna(subset=['Fare']).count()\n",
    "# 値の欠落を埋める\n",
    "train.where(train.Sex == \"male\").Age.min() # 0.41999999999999998 29 80\n",
    "#train.where(train.Sex == \"female\").Age.min() #0.75 27 63\n",
    "#train.Age.fillna(train.Age.median(), inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 57.438435503\n",
      "Epoch: 0002 cost= 38.556251650\n",
      "Epoch: 0003 cost= 27.079147286\n",
      "Epoch: 0004 cost= 22.003307903\n",
      "Epoch: 0005 cost= 17.725240074\n",
      "Epoch: 0006 cost= 18.246781544\n",
      "Epoch: 0007 cost= 10.827384042\n",
      "Epoch: 0008 cost= 10.692609782\n",
      "Epoch: 0009 cost= 8.878264447\n",
      "Epoch: 0010 cost= 8.329427984\n",
      "Optimization Finished!\n",
      "Accuracy: 0.824176\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001 # 学習率 高いとcostの収束が早まる\n",
    "training_epochs = 10 # 学習全体をこのエポック数で区切り、区切りごとにcostを表示する\n",
    "batch_size = 100     # 学習1回ごと( sess.run()ごと )に訓練データをいくつ利用するか\n",
    "display_step = 1     # 1なら毎エポックごとにcostを表示\n",
    "train_size = 800     # 全データの中でいくつ訓練データに回すか\n",
    "step_size = 2000     # 何ステップ学習するか\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 64      # 隠れ層1のユニットの数\n",
    "n_hidden_2 = 64      # 隠れ層2のユニットの数\n",
    "n_input = 8          # 与える変数の数\n",
    "n_classes = 2        # 分類するクラスの数 今回は生き残ったか否かなので2\n",
    "\n",
    "Epoch: 0001 cost= 30.966967532\n",
    "Epoch: 0002 cost= 2.927594726\n",
    "Epoch: 0003 cost= 2.366886817\n",
    "Epoch: 0004 cost= 2.446229219\n",
    "Epoch: 0005 cost= 2.376396438\n",
    "Epoch: 0006 cost= 2.267190577\n",
    "Epoch: 0007 cost= 1.888190902\n",
    "Epoch: 0008 cost= 2.132742506\n",
    "Epoch: 0009 cost= 1.851374626\n",
    "Epoch: 0010 cost= 1.856667134\n",
    "Optimization Finished!\n",
    "Accuracy: 0.835165\n",
    "\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01 # 学習率 高いとcostの収束が早まる\n",
    "training_epochs = 10 # 学習全体をこのエポック数で区切り、区切りごとにcostを表示する\n",
    "batch_size = 100     # 学習1回ごと( sess.run()ごと )に訓練データをいくつ利用するか\n",
    "display_step = 1     # 1なら毎エポックごとにcostを表示\n",
    "train_size = 800     # 全データの中でいくつ訓練データに回すか\n",
    "step_size = 1000     # 何ステップ学習するか\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 64      # 隠れ層1のユニットの数\n",
    "n_hidden_2 = 64      # 隠れ層2のユニットの数\n",
    "n_input = 8          # 与える変数の数\n",
    "n_classes = 2        # 分類するクラスの数 今回は生き残ったか否かなので2\n",
    "Epoch: 0001 cost= 57.438435503\n",
    "Epoch: 0002 cost= 38.556251650\n",
    "Epoch: 0003 cost= 27.079147286\n",
    "Epoch: 0004 cost= 22.003307903\n",
    "Epoch: 0005 cost= 17.725240074\n",
    "Epoch: 0006 cost= 18.246781544\n",
    "Epoch: 0007 cost= 10.827384042\n",
    "Epoch: 0008 cost= 10.692609782\n",
    "Epoch: 0009 cost= 8.878264447\n",
    "Epoch: 0010 cost= 8.329427984\n",
    "Optimization Finished!\n",
    "Accuracy: 0.824176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001 # 学習率 高いとcostの収束が早まる\n",
    "training_epochs = 10 # 学習全体をこのエポック数で区切り、区切りごとにcostを表示する\n",
    "batch_size = 100     # 学習1回ごと( sess.run()ごと )に訓練データをいくつ利用するか\n",
    "display_step = 1     # 1なら毎エポックごとにcostを表示\n",
    "train_size = 800     # 全データの中でいくつ訓練データに回すか\n",
    "step_size = 2000     # 何ステップ学習するか\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 64      # 隠れ層1のユニットの数\n",
    "n_hidden_2 = 64      # 隠れ層2のユニットの数\n",
    "n_input = 8          # 与える変数の数\n",
    "n_classes = 2        # 分類するクラスの数 今回は生き残ったか否かなので2\n",
    "\n",
    "Adagrad\n",
    "\n",
    "Epoch: 0001 cost= 464.975965088\n",
    "Epoch: 0002 cost= 86.857236901\n",
    "Epoch: 0003 cost= 44.344178129\n",
    "Epoch: 0004 cost= 36.259928568\n",
    "Epoch: 0005 cost= 31.819526917\n",
    "Epoch: 0006 cost= 28.236630183\n",
    "Epoch: 0007 cost= 24.917934471\n",
    "Epoch: 0008 cost= 22.810194013\n",
    "Epoch: 0009 cost= 21.187019714\n",
    "Epoch: 0010 cost= 19.626279968\n",
    "Optimization Finished!\n",
    "Accuracy: 0.714286"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
